{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using a pipeline to replace a mask from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.1792745292186737,\n",
      "  'sequence': '<s>HuggingFace is creating a tool that the community uses to '\n",
      "              'solve NLP tasks.</s>',\n",
      "  'token': 3944,\n",
      "  'token_str': 'Ġtool'},\n",
      " {'score': 0.11349418759346008,\n",
      "  'sequence': '<s>HuggingFace is creating a framework that the community uses '\n",
      "              'to solve NLP tasks.</s>',\n",
      "  'token': 7208,\n",
      "  'token_str': 'Ġframework'},\n",
      " {'score': 0.052435532212257385,\n",
      "  'sequence': '<s>HuggingFace is creating a library that the community uses to '\n",
      "              'solve NLP tasks.</s>',\n",
      "  'token': 5560,\n",
      "  'token_str': 'Ġlibrary'},\n",
      " {'score': 0.03493548929691315,\n",
      "  'sequence': '<s>HuggingFace is creating a database that the community uses '\n",
      "              'to solve NLP tasks.</s>',\n",
      "  'token': 8503,\n",
      "  'token_str': 'Ġdatabase'},\n",
      " {'score': 0.02860238403081894,\n",
      "  'sequence': '<s>HuggingFace is creating a prototype that the community uses '\n",
      "              'to solve NLP tasks.</s>',\n",
      "  'token': 17715,\n",
      "  'token_str': 'Ġprototype'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a model and a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nattiya/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/modeling_auto.py:821: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\", return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[  101, 12120,  2050,  8683,  1181,  3584,  1132,  2964,  1190,  1103,\n",
      "          3584,  1152, 27180,   119,  7993,  1172,  1939,  1104,  1103,  1415,\n",
      "          3827,  1156,  1494,   103,  1412,  6302,  2555, 10988,   119,   102]])\n",
      "decoded_sequence:  [CLS] Distilled models are smaller than the models they mimic. Using them instead of the large versions would help [MASK] our carbon footprint. [SEP]\n",
      "(tensor([0]), tensor([23]))\n"
     ]
    }
   ],
   "source": [
    "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "print(\"input: \", input)\n",
    "\n",
    "decoded_sequence = tokenizer.decode(input[0])\n",
    "print(\"decoded_sequence: \", decoded_sequence)\n",
    "\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "print(torch.where(input == tokenizer.mask_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  MaskedLMOutput(loss=None, logits=tensor([[[ -6.6732,  -6.6450,  -6.7923,  ...,  -5.5930,  -5.2783,  -5.6559],\n",
      "         [ -6.3221,  -5.6379,  -5.8990,  ...,  -4.6864,  -4.1499,  -5.3507],\n",
      "         [ -5.9863,  -6.0991,  -5.8089,  ...,  -5.2297,  -4.3015,  -6.5971],\n",
      "         ...,\n",
      "         [ -7.8892,  -7.6718,  -7.6357,  ...,  -6.9083,  -5.5853,  -6.2459],\n",
      "         [-14.7710, -14.2714, -14.1642,  ..., -11.4770, -12.1692, -13.1041],\n",
      "         [-14.3695, -13.9839, -13.6330,  ..., -11.2066, -11.6754, -12.7083]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n",
      "token_logits:  tensor([[[ -6.6732,  -6.6450,  -6.7923,  ...,  -5.5930,  -5.2783,  -5.6559],\n",
      "         [ -6.3221,  -5.6379,  -5.8990,  ...,  -4.6864,  -4.1499,  -5.3507],\n",
      "         [ -5.9863,  -6.0991,  -5.8089,  ...,  -5.2297,  -4.3015,  -6.5971],\n",
      "         ...,\n",
      "         [ -7.8892,  -7.6718,  -7.6357,  ...,  -6.9083,  -5.5853,  -6.2459],\n",
      "         [-14.7710, -14.2714, -14.1642,  ..., -11.4770, -12.1692, -13.1041],\n",
      "         [-14.3695, -13.9839, -13.6330,  ..., -11.2066, -11.6754, -12.7083]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(input)\n",
    "print(\"output: \", output)\n",
    "\n",
    "token_logits = output.logits\n",
    "print(\"token_logits: \", token_logits)\n",
    "\n",
    "# Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, \n",
    "# and the values are the scores attributed to each token. The model gives higher score to tokens it deems probable in that context.\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Retrieve the top 5 tokens using the PyTorch topk\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
